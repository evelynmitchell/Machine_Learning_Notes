# Language Tasks

## next sentence prediction

Definition:

Models: BERT "BERT was pretrained on two tasks: language modeling (15% of tokens were masked and BERT was trained to predict them from context) and next sentence prediction (BERT was trained to predict if a chosen next sentence was probable or not given the first sentence). As a result of the training process, BERT learns contextual embeddings for words. After pretraining, which is computationally expensive, BERT can be finetuned with fewer resources on smaller datasets to optimize its performance on specific tasks.[1][6]"[Wikipedia](https://en.wikipedia.org/wiki/BERT_(language_model))

References: [1] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

## Template completion

Definition:

Models: GPT-3 "GPT-3 is a language model that uses a Transformer architecture. It is trained on a large corpus of text and can be used to extract features from text. It is used in many NLP tasks, such as question answering, text classification, and text generation. 2020. "The original English-language GPT-3 has three models:[1] (1) the GPT-3: 175 billion parameters, (2) the GPT-3: 175 billion parameters, and (3) the GPT-3: 175 billion parameters. Both models are pre-trained from unlabeled data extracted from the BooksCorpus[4] with 800M words and English Wikipedia with 2,500M words. " [Wikipedia](https://en.wikipedia.org/wiki/GPT-3)

References: [1] [OpenAI GPT-3](https://openai.com/blog/gpt-3-apps/)
[2] [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

## language modeling

Definition:

Models: BERT "BERT was pretrained on two tasks: language modeling (15% of tokens were masked and BERT was trained to predict them from context) and next sentence prediction (BERT was trained to predict if a chosen next sentence was probable or not given the first sentence). As a result of the training process, BERT learns contextual embeddings for words. After pretraining, which is computationally expensive, BERT can be finetuned with fewer resources on smaller datasets to optimize its performance on specific tasks.[1][6]"https://en.wikipedia.org/wiki/BERT_(language_model)

References: [1] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

# text classification

Definition:

Models: 

# question answering

Definition:

Models: 

# text generation

Definition:

Models:

