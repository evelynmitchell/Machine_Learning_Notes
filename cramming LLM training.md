# cramming LLM training

source: [cramming](https://github.com/JonasGeiping/cramming)

type: Language

keywords: small memory, BERT, LLM, optimize, training, mlops

paper: [Cramming: Training a Language Model on a Single GPU in One Day](https://arxiv.org/abs/2212.14034)

replicated: not yet

papers with code: (https://paperswithcode.com/paper/cramming-training-a-language-model-on-a)

tasks: language modeling, next sentence prediction 

## Summary

A fast and small to train implementation of [BERT](https://arxiv.org/abs/1810.04805) with a memory of 1.5GB. The model is trained on a single GPU in one day.

[BERT](https://en.wikipedia.org/wiki/BERT_(language_model) is a Bidirectional Embedding Representations from Transformers. It is a language model that uses a Transformer architecture. It is trained on a large corpus of text and can be used to extract features from text. It is used in many NLP tasks, such as question answering, text classification, and text generation. 2018. "The original English-language BERT has two models:[1] (1) the BERT_BASE: 12 encoders with 12 bidirectional self-attention heads, and (2) the BERT_LARGE: 24 encoders with 16 bidirectional self-attention heads. Both models are pre-trained from unlabeled data extracted from the BooksCorpus[4] with 800M words and English Wikipedia with 2,500M words. " [Wikipedia](https://en.wikipedia.org/wiki/BERT_(language_model))
